{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16d1e9a6",
   "metadata": {},
   "source": [
    "# Final Project | Stage 2\n",
    "## Evaluating and Comparing Different Machine Learning Models Performance on Autoimmune Disease Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5437703",
   "metadata": {},
   "source": [
    "### Imports, Global Settings, and Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "285dfd1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12f35f910>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Set random seed to ensure reproducible results across multiple runs\n",
    "# This is critical for scientific validity - same seed = same results\n",
    "RANDOM_STATE = 42\n",
    "random.seed(RANDOM_STATE)           # Python's random module\n",
    "np.random.seed(RANDOM_STATE)        # NumPy's random number generator\n",
    "torch.manual_seed(RANDOM_STATE)     # PyTorch's random number generator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f044ea8",
   "metadata": {},
   "source": [
    "### Data Import and Preprocessing\n",
    "This section:\n",
    "- Loads the dataset\n",
    "- Encodes categorical variables\n",
    "- Scales numerical features\n",
    "- Splits data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01f1c5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Preprocessing complete\n",
      "Training samples: 8198\n",
      "Testing samples: 3514\n",
      "Number of features: 77\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "data = pd.read_csv(\n",
    "\"/Users/Caitlynrose/Machine_Learning/Final_Project/health_data/Final_Balanced_Autoimmune_Disorder_Dataset.csv\"\n",
    ")\n",
    "\n",
    "# Encode Diagnosis labels (e.g., \"Rheumatoid arthritis\"=0, \"SLE\"=1, etc.)\n",
    "# We save the encoder so we can convert predictions back to disease names later\n",
    "diagnosis_encoder = LabelEncoder()\n",
    "data[\"Gender\"] = diagnosis_encoder.fit_transform(data[\"Gender\"])\n",
    "data[\"Diagnosis\"] = diagnosis_encoder.fit_transform(data[\"Diagnosis\"])\n",
    "diagnosis_labels = diagnosis_encoder.classes_  # Save original disease names\n",
    "num_classes = len(diagnosis_labels)            # Number of different diseases\n",
    "\n",
    "# X = features (input variables used to make predictions)\n",
    "# y = target (the diagnosis we're trying to predict)\n",
    "X = data.drop(columns=[\"Patient_ID\", \"Diagnosis\"])  # Remove ID and target\n",
    "y = data[\"Diagnosis\"]                                # Keep only target\n",
    "\n",
    "# Standardize features by removing mean and scaling to unit variance\n",
    "# This is critical for:\n",
    "# 1. Logistic Regression - sensitive to feature scales\n",
    "# 2. Neural Networks - helps with convergence and training stability\n",
    "# Random Forest doesn't need scaling but it doesn't hurt\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into training (70%) and testing (30%) sets\n",
    "# - Training set: used to train models and tune hyperparameters\n",
    "# - Testing set: held out to evaluate final performance (unseen data)\n",
    "# stratify=y ensures both sets have same proportion of each disease\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.3, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Preprocessing complete\")\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Testing samples: {X_test.shape[0]}\")\n",
    "print(f\"Number of features: {X_train.shape[1]}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bf0a8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HELPER FUNCTION FOR RESULTS DISPLAY\n",
    "# ============================================================================\n",
    "def print_model_results(model_name, best_params, train_acc, test_acc, f1, \n",
    "                        y_true, y_pred, labels, num_samples=10):\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{model_name.upper()} - RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Display all hyperparameters that were optimized\n",
    "    print(\"\\nüìä Best Hyperparameters:\")\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"   ‚Ä¢ {param}: {value}\")\n",
    "    \n",
    "    # Display performance metrics\n",
    "    # Training accuracy: how well the model memorizes training data\n",
    "    # Testing accuracy: how well the model generalizes\n",
    "    # F1 score: balances precision and recall\n",
    "    print(\"\\nüìà Performance Metrics:\")\n",
    "    print(f\"   ‚Ä¢ Training Accuracy:    {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
    "    print(f\"   ‚Ä¢ Testing Accuracy:     {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "    print(f\"   ‚Ä¢ F1 Score (Weighted):  {f1:.4f}\")\n",
    "    \n",
    "    # Show sample predictions to give intuition about model performance\n",
    "    # This helps us see what kinds of mistakes the model is making\n",
    "    print(f\"\\nüîç Sample Predictions (First {num_samples}):\")\n",
    "    print(f\"   {'Predicted':<40} {'Actual':<40} {'Match'}\")\n",
    "    print(f\"   {'-'*40} {'-'*40} {'-'*5}\")\n",
    "    \n",
    "    matches = 0\n",
    "    for i in range(min(num_samples, len(y_pred))):\n",
    "        # Convert numerical predictions back to disease names\n",
    "        pred_label = labels[y_pred[i]]\n",
    "        true_label = labels[y_true.iloc[i] if hasattr(y_true, 'iloc') else y_true[i]]\n",
    "        match = \"‚úì\" if pred_label == true_label else \"‚úó\"\n",
    "        if pred_label == true_label:\n",
    "            matches += 1\n",
    "        print(f\"   {pred_label:<40} {true_label:<40} {match}\")\n",
    "    \n",
    "    print(f\"\\n   Sample accuracy: {matches}/{num_samples}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2764f8dd",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac06580f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODEL 1: LOGISTIC REGRESSION\n",
      "================================================================================\n",
      "\n",
      "üîç Starting comprehensive grid search...\n",
      "\n",
      "Fitting 5 folds for each of 63 candidates, totalling 315 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Logistic Regression training complete\n",
      "\n",
      "================================================================================\n",
      "LOGISTIC REGRESSION - RESULTS\n",
      "================================================================================\n",
      "\n",
      "üìä Best Hyperparameters:\n",
      "   ‚Ä¢ C: 0.001\n",
      "   ‚Ä¢ max_iter: 1000\n",
      "   ‚Ä¢ penalty: l2\n",
      "   ‚Ä¢ solver: lbfgs\n",
      "\n",
      "üìà Performance Metrics:\n",
      "   ‚Ä¢ Training Accuracy:    0.4525 (45.25%)\n",
      "   ‚Ä¢ Testing Accuracy:     0.4254 (42.54%)\n",
      "   ‚Ä¢ F1 Score (Weighted):  0.4236\n",
      "\n",
      "üîç Sample Predictions (First 10):\n",
      "   Predicted                                Actual                                   Match\n",
      "   ---------------------------------------- ---------------------------------------- -----\n",
      "   Graves' disease                          Sj√∂gren syndrome                         ‚úó\n",
      "   Systemic lupus erythematosus (SLE)       Autoimmune orchitis                      ‚úó\n",
      "   Systemic lupus erythematosus (SLE)       Systemic lupus erythematosus (SLE)       ‚úì\n",
      "   Sj√∂gren syndrome                         Sj√∂gren syndrome                         ‚úì\n",
      "   Graves' disease                          Graves' disease                          ‚úì\n",
      "   Autoimmune orchitis                      Sj√∂gren syndrome                         ‚úó\n",
      "   Systemic lupus erythematosus (SLE)       Systemic lupus erythematosus (SLE)       ‚úì\n",
      "   Graves' disease                          Rheumatoid arthritis                     ‚úó\n",
      "   Sj√∂gren syndrome                         Sj√∂gren syndrome                         ‚úì\n",
      "   Autoimmune orchitis                      Rheumatoid arthritis                     ‚úó\n",
      "\n",
      "   Sample accuracy: 5/10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MODEL 1: LOGISTIC REGRESSION\n",
    "# ============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL 1: LOGISTIC REGRESSION\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nüîç Starting comprehensive grid search...\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOGISTIC REGRESSION HYPERPARAMETER GRID\n",
    "# ============================================================================\n",
    "# We test different combinations of hyperparameters to find the best model\n",
    "param_grid_lr = {\n",
    "    # C: Inverse of regularization strength (smaller = more regularization)\n",
    "    # Regularization prevents overfitting by penalizing large coefficients\n",
    "    \"C\": [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    \n",
    "    # solver: Algorithm used to optimize the model\n",
    "    # Different solvers work better for different data sizes/types\n",
    "    \"solver\": [\"lbfgs\", \"liblinear\", \"saga\"],\n",
    "    \n",
    "    # penalty: Type of regularization to apply\n",
    "    # L2 (ridge) penalty helps prevent overfitting\n",
    "    \"penalty\": [\"l2\"],\n",
    "    \n",
    "    # max_iter: Maximum iterations for solver to converge\n",
    "    # More iterations = more time but potentially better solution\n",
    "    \"max_iter\": [1000, 2000, 3000]\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# GRID SEARCH WITH CROSS-VALIDATION\n",
    "# ============================================================================\n",
    "# GridSearchCV tests every combination of hyperparameters\n",
    "# cv=5 means 5-fold cross-validation: split training data into 5 parts,\n",
    "# train on 4, validate on 1, repeat 5 times, average results\n",
    "# This gives us a robust estimate of how well each configuration performs\n",
    "grid_lr = GridSearchCV(\n",
    "    estimator=LogisticRegression(random_state=RANDOM_STATE),\n",
    "    param_grid=param_grid_lr,\n",
    "    scoring=\"accuracy\",      # Optimize for accuracy\n",
    "    cv=5,                    # 5-fold cross-validation\n",
    "    n_jobs=-1,              # Use all CPU cores for parallel processing\n",
    "    verbose=1               # Print progress\n",
    ")\n",
    "\n",
    "# Train the model and find best hyperparameters\n",
    "grid_lr.fit(X_train, y_train)\n",
    "best_lr = grid_lr.best_estimator_  # Get the best model\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATE LOGISTIC REGRESSION\n",
    "# ============================================================================\n",
    "# Generate predictions on both training and testing sets\n",
    "lr_train_preds = best_lr.predict(X_train)\n",
    "lr_test_preds = best_lr.predict(X_test)\n",
    "\n",
    "# Calculate performance metrics\n",
    "lr_train_acc = accuracy_score(y_train, lr_train_preds)  # Training accuracy\n",
    "lr_test_acc = accuracy_score(y_test, lr_test_preds)      # Testing accuracy (most important!)\n",
    "lr_f1 = f1_score(y_test, lr_test_preds, average='weighted')  # F1 score\n",
    "\n",
    "print(\"\\n‚úì Logistic Regression training complete\\n\")\n",
    "\n",
    "# Display all results in a formatted way\n",
    "print_model_results(\n",
    "    \"Logistic Regression\",\n",
    "    grid_lr.best_params_,\n",
    "    lr_train_acc,\n",
    "    lr_test_acc,\n",
    "    lr_f1,\n",
    "    y_test,\n",
    "    lr_test_preds,\n",
    "    diagnosis_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f166e3c1",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "627d2ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODEL 2: RANDOM FOREST\n",
      "================================================================================\n",
      "\n",
      "üå≤ Starting condensed grid search...\n",
      "\n",
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n",
      "\n",
      "‚úì Random Forest training complete\n",
      "\n",
      "================================================================================\n",
      "RANDOM FOREST - RESULTS\n",
      "================================================================================\n",
      "\n",
      "üìä Best Hyperparameters:\n",
      "   ‚Ä¢ bootstrap: True\n",
      "   ‚Ä¢ max_depth: None\n",
      "   ‚Ä¢ max_features: sqrt\n",
      "   ‚Ä¢ min_samples_leaf: 4\n",
      "   ‚Ä¢ min_samples_split: 2\n",
      "   ‚Ä¢ n_estimators: 100\n",
      "\n",
      "üìà Performance Metrics:\n",
      "   ‚Ä¢ Training Accuracy:    0.9790 (97.90%)\n",
      "   ‚Ä¢ Testing Accuracy:     0.9772 (97.72%)\n",
      "   ‚Ä¢ F1 Score (Weighted):  0.9732\n",
      "\n",
      "üîç Sample Predictions (First 10):\n",
      "   Predicted                                Actual                                   Match\n",
      "   ---------------------------------------- ---------------------------------------- -----\n",
      "   Sj√∂gren syndrome                         Sj√∂gren syndrome                         ‚úì\n",
      "   Autoimmune orchitis                      Autoimmune orchitis                      ‚úì\n",
      "   Systemic lupus erythematosus (SLE)       Systemic lupus erythematosus (SLE)       ‚úì\n",
      "   Sj√∂gren syndrome                         Sj√∂gren syndrome                         ‚úì\n",
      "   Graves' disease                          Graves' disease                          ‚úì\n",
      "   Sj√∂gren syndrome                         Sj√∂gren syndrome                         ‚úì\n",
      "   Systemic lupus erythematosus (SLE)       Systemic lupus erythematosus (SLE)       ‚úì\n",
      "   Rheumatoid arthritis                     Rheumatoid arthritis                     ‚úì\n",
      "   Sj√∂gren syndrome                         Sj√∂gren syndrome                         ‚úì\n",
      "   Rheumatoid arthritis                     Rheumatoid arthritis                     ‚úì\n",
      "\n",
      "   Sample accuracy: 10/10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MODEL 2: RANDOM FOREST\n",
    "# ============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL 2: RANDOM FOREST\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nüå≤ Starting condensed grid search...\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# RANDOM FOREST HYPERPARAMETER GRID (CONDENSED)\n",
    "# ============================================================================\n",
    "# Reduce the number of hyperparameter combinations to speed up the search\n",
    "param_grid_rf = {\n",
    "    \"n_estimators\": [100, 200],  \n",
    "    \"max_depth\": [None, 20, 40],  \n",
    "    \"min_samples_split\": [2, 10], \n",
    "    \"min_samples_leaf\": [1, 4],  \n",
    "    \"max_features\": [\"sqrt\", \"log2\"],  \n",
    "    \"bootstrap\": [True] \n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# GRID SEARCH WITH CROSS-VALIDATION\n",
    "# ============================================================================\n",
    "grid_rf = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=RANDOM_STATE),\n",
    "    param_grid=param_grid_rf,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=3,  # Reduce cross-validation folds to 3\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train and find best hyperparameters\n",
    "grid_rf.fit(X_train, y_train)\n",
    "best_rf = grid_rf.best_estimator_\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATE RANDOM FOREST\n",
    "# ============================================================================\n",
    "rf_train_preds = best_rf.predict(X_train)\n",
    "rf_test_preds = best_rf.predict(X_test)\n",
    "\n",
    "rf_train_acc = accuracy_score(y_train, rf_train_preds)\n",
    "rf_test_acc = accuracy_score(y_test, rf_test_preds)\n",
    "rf_f1 = f1_score(y_test, rf_test_preds, average='weighted')\n",
    "\n",
    "print(\"\\n‚úì Random Forest training complete\\n\")\n",
    "\n",
    "print_model_results(\n",
    "    \"Random Forest\",\n",
    "    grid_rf.best_params_,\n",
    "    rf_train_acc,\n",
    "    rf_test_acc,\n",
    "    rf_f1,\n",
    "    y_test,\n",
    "    rf_test_preds,\n",
    "    diagnosis_labels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cf655b",
   "metadata": {},
   "source": [
    "### Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a035888f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODEL 3: NEURAL NETWORK (COMPLEX + REGULARIZATION)\n",
      "================================================================================\n",
      "\n",
      "Strategy: Start with complex architecture, tune regularization\n",
      "\n",
      "üß† Starting comprehensive hyperparameter search...\n",
      "\n",
      "Performing 50 trials...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10/50 complete | Best val acc so far: 0.9793\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 248\u001b[0m\n\u001b[1;32m    239\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_layers\u001b[39m\u001b[38;5;124m\"\u001b[39m: random\u001b[38;5;241m.\u001b[39mchoice(nn_search_space[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_layers\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m\"\u001b[39m: random\u001b[38;5;241m.\u001b[39mchoice(nn_search_space[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: random\u001b[38;5;241m.\u001b[39mchoice(nn_search_space[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    245\u001b[0m }\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# Train model with this configuration\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m _, _, val_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_eval_nn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_nn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val_nn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_nn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_nn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\n\u001b[1;32m    250\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# Save results\u001b[39;00m\n\u001b[1;32m    253\u001b[0m results\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: val_acc})\n",
      "Cell \u001b[0;32mIn[15], line 154\u001b[0m, in \u001b[0;36mtrain_and_eval_nn\u001b[0;34m(X_train, X_val, y_train, y_val, hidden_layers, dropout, lr, batch_size, weight_decay, epochs)\u001b[0m\n\u001b[1;32m    151\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_train_t[idx])\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# Backward pass: compute gradients\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# Update weights based on gradients\u001b[39;00m\n\u001b[1;32m    157\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MODEL 3: COMPLEX NEURAL NETWORK WITH REGULARIZATION\n",
    "# ============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL 3: NEURAL NETWORK (COMPLEX + REGULARIZATION)\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nStrategy: Start with complex architecture, tune regularization\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# NEURAL NETWORK CLASS DEFINITION\n",
    "# ============================================================================\n",
    "class ComplexAutoimmuneNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Complex neural network with multiple hidden layers and dropout regularization\n",
    "    \n",
    "    Professor's guidance: Make the network complex, then control overfitting\n",
    "    through regularization (dropout + L2 weight decay)\n",
    "    \n",
    "    Architecture:\n",
    "    - Multiple hidden layers with ReLU activation\n",
    "    - Dropout after each hidden layer to prevent overfitting\n",
    "    - Final output layer for classification\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_layers, dropout, num_classes):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_dim : int\n",
    "            Number of input features\n",
    "        hidden_layers : list of int\n",
    "            List specifying neurons in each hidden layer (e.g., [512, 256, 128])\n",
    "        dropout : float\n",
    "            Dropout probability (0.0 to 1.0) - higher = more regularization\n",
    "        num_classes : int\n",
    "            Number of output classes (diseases to predict)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        # Build the network layer by layer\n",
    "        for h in hidden_layers:\n",
    "            # Linear transformation: y = Wx + b\n",
    "            layers.append(nn.Linear(prev_dim, h))\n",
    "            \n",
    "            # ReLU activation: adds non-linearity, allows network to learn complex patterns\n",
    "            # ReLU(x) = max(0, x) - simple but effective\n",
    "            layers.append(nn.ReLU())\n",
    "            \n",
    "            # Dropout: randomly sets neurons to 0 during training\n",
    "            # This prevents co-adaptation and forces redundancy in the network\n",
    "            # Key regularization technique to prevent overfitting\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            \n",
    "            prev_dim = h\n",
    "        \n",
    "        # Output layer: maps final hidden layer to class predictions\n",
    "        # No activation here - we'll use CrossEntropyLoss which includes softmax\n",
    "        layers.append(nn.Linear(prev_dim, num_classes))\n",
    "        \n",
    "        # Combine all layers into a sequential model\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass: compute predictions from inputs\"\"\"\n",
    "        return self.model(x)\n",
    "\n",
    "# ============================================================================\n",
    "# NEURAL NETWORK TRAINING FUNCTION\n",
    "# ============================================================================\n",
    "def train_and_eval_nn(X_train, X_val, y_train, y_val, hidden_layers, \n",
    "                       dropout, lr, batch_size, weight_decay, epochs=200):\n",
    "    \"\"\"\n",
    "    Train and evaluate neural network with given hyperparameters\n",
    "    \n",
    "    This function:\n",
    "    1. Converts data to PyTorch tensors\n",
    "    2. Initializes the network\n",
    "    3. Trains using mini-batch gradient descent\n",
    "    4. Evaluates on validation set\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train, X_val : array\n",
    "        Training and validation features\n",
    "    y_train, y_val : array\n",
    "        Training and validation labels\n",
    "    hidden_layers : list\n",
    "        Network architecture\n",
    "    dropout : float\n",
    "        Dropout probability for regularization\n",
    "    lr : float\n",
    "        Learning rate for optimizer\n",
    "    batch_size : int\n",
    "        Number of samples per gradient update\n",
    "    weight_decay : float\n",
    "        L2 regularization strength (applied to weights in optimizer)\n",
    "    epochs : int\n",
    "        Number of complete passes through training data\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    model : trained neural network\n",
    "    train_acc : training accuracy\n",
    "    val_acc : validation accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    # ========================================================================\n",
    "    # CONVERT DATA TO PYTORCH TENSORS\n",
    "    # ========================================================================\n",
    "    # PyTorch requires data in tensor format (similar to NumPy arrays)\n",
    "    X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "    X_val_t = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_train_t = torch.tensor(y_train.values if hasattr(y_train, 'values') else y_train, dtype=torch.long)\n",
    "    y_val_t = torch.tensor(y_val.values if hasattr(y_val, 'values') else y_val, dtype=torch.long)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # INITIALIZE MODEL AND TRAINING COMPONENTS\n",
    "    # ========================================================================\n",
    "    model = ComplexAutoimmuneNet(X_train.shape[1], hidden_layers, dropout, num_classes)\n",
    "    \n",
    "    # Adam optimizer: adaptive learning rate method\n",
    "    # weight_decay: L2 regularization penalty on weights (prevents large weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # CrossEntropyLoss: standard loss for multi-class classification\n",
    "    # Combines LogSoftmax and NLLLoss - computes how wrong predictions are\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # TRAINING LOOP\n",
    "    # ========================================================================\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set model to training mode (enables dropout)\n",
    "        \n",
    "        # Shuffle training data each epoch for better generalization\n",
    "        perm = torch.randperm(X_train_t.size(0))\n",
    "        \n",
    "        # Mini-batch gradient descent: update weights after each batch\n",
    "        # Faster and often better than using entire dataset at once\n",
    "        for i in range(0, X_train_t.size(0), batch_size):\n",
    "            idx = perm[i:i + batch_size]\n",
    "            \n",
    "            # Zero gradients from previous step\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass: compute predictions\n",
    "            outputs = model(X_train_t[idx])\n",
    "            \n",
    "            # Compute loss: how wrong are our predictions?\n",
    "            loss = criterion(outputs, y_train_t[idx])\n",
    "            \n",
    "            # Backward pass: compute gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update weights based on gradients\n",
    "            optimizer.step()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # EVALUATION\n",
    "    # ========================================================================\n",
    "    model.eval()  # Set to evaluation mode (disables dropout)\n",
    "    \n",
    "    # Don't compute gradients during evaluation (saves memory and computation)\n",
    "    with torch.no_grad():\n",
    "        # Get predictions: choose class with highest score\n",
    "        train_preds = torch.argmax(model(X_train_t), dim=1)\n",
    "        val_preds = torch.argmax(model(X_val_t), dim=1)\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    train_acc = accuracy_score(y_train_t, train_preds)\n",
    "    val_acc = accuracy_score(y_val_t, val_preds)\n",
    "    \n",
    "    return model, train_acc, val_acc\n",
    "\n",
    "# ============================================================================\n",
    "# NEURAL NETWORK HYPERPARAMETER SEARCH\n",
    "# ============================================================================\n",
    "print(\"üß† Starting comprehensive hyperparameter search...\\n\")\n",
    "\n",
    "# Create validation split from training data\n",
    "# This is separate from the test set - used only for hyperparameter tuning\n",
    "# Test set remains completely untouched until final evaluation\n",
    "X_train_nn, X_val_nn, y_train_nn, y_val_nn = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=RANDOM_STATE, stratify=y_train\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# DEFINE SEARCH SPACE\n",
    "# ============================================================================\n",
    "# Following professor's advice: Start with COMPLEX architectures\n",
    "# These are deep (4-5 layers) and wide (512-2048 neurons)\n",
    "nn_search_space = {\n",
    "    # Complex architectures - various depths and widths\n",
    "    \"hidden_layers\": [\n",
    "        [512, 256, 128, 64],           # Deep pyramid\n",
    "        [1024, 512, 256, 128],         # Wider deep pyramid\n",
    "        [512, 512, 256, 128, 64],      # Extra deep\n",
    "        [1024, 512, 512, 256, 128],    # Extra deep and wide\n",
    "        [1024, 1024, 512],             # Very wide\n",
    "        [2048, 1024, 512],             # Extremely wide\n",
    "        [512, 256, 256, 128],          # Repeated middle layers\n",
    "        [1024, 512, 256, 256, 128],    # Complex mixed\n",
    "    ],\n",
    "    \n",
    "    # Dropout: PRIMARY regularization method\n",
    "    # Randomly drops neurons during training to prevent overfitting\n",
    "    # Higher values = more regularization = less overfitting but potentially underfitting\n",
    "    \"dropout\": [0.1, 0.2, 0.3, 0.4, 0.5, 0.6],\n",
    "    \n",
    "    # Weight decay: SECONDARY regularization (L2 penalty on weights)\n",
    "    # Prevents weights from growing too large\n",
    "    # Works together with dropout for strong regularization\n",
    "    \"weight_decay\": [1e-5, 1e-4, 1e-3, 1e-2],\n",
    "    \n",
    "    # Learning rate: controls how big each gradient descent step is\n",
    "    # Too high = unstable training, too low = slow convergence\n",
    "    \"lr\": [1e-2, 5e-3, 1e-3, 5e-4, 1e-4],\n",
    "    \n",
    "    # Batch size: number of samples before updating weights\n",
    "    # Smaller = more updates but noisier, larger = fewer but stabler updates\n",
    "    \"batch_size\": [16, 32, 64, 128]\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# RANDOMIZED HYPERPARAMETER SEARCH\n",
    "# ============================================================================\n",
    "# We use random search instead of grid search because:\n",
    "# 1. Grid search would take too long (thousands of combinations)\n",
    "# 2. Random search is more efficient - can find good solutions faster\n",
    "# 3. Allows us to explore diverse configurations\n",
    "num_trials = 50  # Test 50 random configurations\n",
    "results = []\n",
    "\n",
    "print(f\"Performing {num_trials} trials...\\n\")\n",
    "\n",
    "for trial in range(num_trials):\n",
    "    # Randomly sample one configuration\n",
    "    config = {\n",
    "        \"hidden_layers\": random.choice(nn_search_space[\"hidden_layers\"]),\n",
    "        \"dropout\": random.choice(nn_search_space[\"dropout\"]),\n",
    "        \"weight_decay\": random.choice(nn_search_space[\"weight_decay\"]),\n",
    "        \"lr\": random.choice(nn_search_space[\"lr\"]),\n",
    "        \"batch_size\": random.choice(nn_search_space[\"batch_size\"])\n",
    "    }\n",
    "    \n",
    "    # Train model with this configuration\n",
    "    _, _, val_acc = train_and_eval_nn(\n",
    "        X_train_nn, X_val_nn, y_train_nn, y_val_nn, **config, epochs=150\n",
    "    )\n",
    "    \n",
    "    # Save results\n",
    "    results.append({**config, \"val_accuracy\": val_acc})\n",
    "    \n",
    "    # Print progress every 10 trials\n",
    "    if (trial + 1) % 10 == 0:\n",
    "        best_so_far = max(results, key=lambda x: x[\"val_accuracy\"])[\"val_accuracy\"]\n",
    "        print(f\"Trial {trial + 1}/{num_trials} complete | Best val acc so far: {best_so_far:.4f}\")\n",
    "\n",
    "# Find the best configuration from all trials\n",
    "best_nn_config = max(results, key=lambda x: x[\"val_accuracy\"])\n",
    "\n",
    "print(f\"\\n‚úì Hyperparameter search complete!\")\n",
    "print(f\"Best validation accuracy: {best_nn_config['val_accuracy']:.4f}\")\n",
    "print(f\"\\nBest architecture: {best_nn_config['hidden_layers']}\")\n",
    "print(f\"Best dropout: {best_nn_config['dropout']}\")\n",
    "print(f\"Best weight_decay: {best_nn_config['weight_decay']}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# TRAIN FINAL NEURAL NETWORK\n",
    "# ============================================================================\n",
    "print(\"üöÄ Training final Neural Network with best hyperparameters...\\n\")\n",
    "\n",
    "# Now train on FULL training set with the best hyperparameters\n",
    "# Use more epochs since this is our final model\n",
    "final_model, _, _ = train_and_eval_nn(\n",
    "    X_train, X_test, y_train, y_test,\n",
    "    hidden_layers=best_nn_config[\"hidden_layers\"],\n",
    "    dropout=best_nn_config[\"dropout\"],\n",
    "    weight_decay=best_nn_config[\"weight_decay\"],\n",
    "    lr=best_nn_config[\"lr\"],\n",
    "    batch_size=best_nn_config[\"batch_size\"],\n",
    "    epochs=250  # More training for final model\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATE FINAL NEURAL NETWORK\n",
    "# ============================================================================\n",
    "final_model.eval()  # Set to evaluation mode\n",
    "with torch.no_grad():\n",
    "    X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "    X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "    nn_train_preds = torch.argmax(final_model(X_train_t), dim=1).numpy()\n",
    "    nn_test_preds = torch.argmax(final_model(X_test_t), dim=1).numpy()\n",
    "\n",
    "# Calculate final metrics\n",
    "nn_train_acc = accuracy_score(y_train, nn_train_preds)\n",
    "nn_test_acc = accuracy_score(y_test, nn_test_preds)\n",
    "nn_f1 = f1_score(y_test, nn_test_preds, average='weighted')\n",
    "\n",
    "print(\"‚úì Final model training complete\\n\")\n",
    "\n",
    "# Prepare parameters dictionary for display\n",
    "best_params_nn = {\n",
    "    \"architecture\": best_nn_config[\"hidden_layers\"],\n",
    "    \"dropout\": best_nn_config[\"dropout\"],\n",
    "    \"weight_decay (L2)\": best_nn_config[\"weight_decay\"],\n",
    "    \"learning_rate\": best_nn_config[\"lr\"],\n",
    "    \"batch_size\": best_nn_config[\"batch_size\"],\n",
    "    \"epochs\": 250\n",
    "}\n",
    "\n",
    "print_model_results(\n",
    "    \"Neural Network (Complex + Regularization)\",\n",
    "    best_params_nn,\n",
    "    nn_train_acc,\n",
    "    nn_test_acc,\n",
    "    nn_f1,\n",
    "    y_test,\n",
    "    nn_test_preds,\n",
    "    diagnosis_labels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702241d6",
   "metadata": {},
   "source": [
    "### FINAL COMPARISON AND BEST MODEL IDENTIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78912552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FINAL COMPARISON AND BEST MODEL IDENTIFICATION\n",
    "# ============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"FINAL MODEL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Display side-by-side comparison of all three models\n",
    "print(f\"\\n{'Model':<35} {'Train Acc':<12} {'Test Acc':<12} {'F1 Score':<12}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Logistic Regression':<35} {lr_train_acc:<12.4f} {lr_test_acc:<12.4f} {lr_f1:<12.4f}\")\n",
    "print(f\"{'Random Forest':<35} {rf_train_acc:<12.4f} {rf_test_acc:<12.4f} {rf_f1:<12.4f}\")\n",
    "print(f\"{'Neural Network (Complex+Reg)':<35} {nn_train_acc:<12.4f} {nn_test_acc:<12.4f} {nn_f1:<12.4f}\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# IDENTIFY THE BEST MODEL\n",
    "# ============================================================================\n",
    "# We select the best model based on TEST accuracy (not training!)\n",
    "# Test accuracy tells us how well the model generalizes to unseen data\n",
    "models = [\n",
    "    (\"Logistic Regression\", lr_test_acc, lr_f1),\n",
    "    (\"Random Forest\", rf_test_acc, rf_f1),\n",
    "    (\"Neural Network\", nn_test_acc, nn_f1)\n",
    "]\n",
    "\n",
    "# Find model with highest test accuracy\n",
    "best_model = max(models, key=lambda x: x[1])\n",
    "\n",
    "# Display the winner!\n",
    "print(\"=\" * 80)\n",
    "print(\"üèÜ BEST MODEL IDENTIFIED\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nModel: {best_model[0]}\")\n",
    "print(f\"Test Accuracy: {best_model[1]:.4f} ({best_model[1]*100:.2f}%)\")\n",
    "print(f\"F1 Score: {best_model[2]:.4f}\")\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e767b8",
   "metadata": {},
   "source": [
    "## Applying Model to My Own Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65eac1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PERSONAL HEALTH DATA PREDICTION\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nApplying the best model to your personal health data...\\n\")\n",
    "\n",
    "\n",
    "your_health_data = pd.read_csv(\"/Users/Caitlynrose/Machine_Learning/Final_Project/health_data/my_health_data.csv\")\n",
    "\n",
    "# ============================================================================\n",
    "# PREPARE YOUR DATA FOR PREDICTION\n",
    "# ============================================================================\n",
    "# Convert your data dictionary to a DataFrame (same format as training data)\n",
    "your_data_df = pd.DataFrame([your_health_data])\n",
    "\n",
    "# Encode Gender the same way we did for training data\n",
    "# IMPORTANT: Must use the same encoder that was fit on training data\n",
    "your_data_df[\"Gender\"] = gender_encoder.transform(your_data_df[\"Gender\"])\n",
    "\n",
    "# Make sure columns are in the same order as training data\n",
    "# Get the feature names from the original training data (before scaling)\n",
    "feature_names = X.columns if hasattr(X, 'columns') else [f\"Feature_{i}\" for i in range(X.shape[1])]\n",
    "\n",
    "# Reorder your data to match training data column order\n",
    "# If you're missing any features, this will error - make sure to include all!\n",
    "try:\n",
    "    your_data_ordered = your_data_df[feature_names]\n",
    "except KeyError as e:\n",
    "    print(f\"\\n‚ö†Ô∏è ERROR: Missing feature in your data: {e}\")\n",
    "    print(f\"\\nRequired features: {list(feature_names)}\")\n",
    "    print(f\"Your features: {list(your_data_df.columns)}\")\n",
    "    print(\"\\nPlease add all required features to your_health_data dictionary\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SCALE YOUR DATA\n",
    "# ============================================================================\n",
    "# CRITICAL: Must use the SAME scaler that was fit on training data\n",
    "# This ensures your data is on the same scale as the training data\n",
    "your_data_scaled = scaler.transform(your_data_ordered)\n",
    "\n",
    "# ============================================================================\n",
    "# MAKE PREDICTIONS WITH ALL THREE MODELS\n",
    "# ============================================================================\n",
    "print(\"üîÆ Generating predictions from all three models...\\n\")\n",
    "\n",
    "# Logistic Regression Prediction\n",
    "lr_your_pred = best_lr.predict(your_data_scaled)[0]\n",
    "lr_your_pred_proba = best_lr.predict_proba(your_data_scaled)[0]\n",
    "\n",
    "# Random Forest Prediction\n",
    "rf_your_pred = best_rf.predict(your_data_scaled)[0]\n",
    "rf_your_pred_proba = best_rf.predict_proba(your_data_scaled)[0]\n",
    "\n",
    "# Neural Network Prediction\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    your_data_tensor = torch.tensor(your_data_scaled, dtype=torch.float32)\n",
    "    nn_your_logits = final_model(your_data_tensor)\n",
    "    nn_your_pred = torch.argmax(nn_your_logits, dim=1).item()\n",
    "    # Calculate probabilities using softmax\n",
    "    nn_your_pred_proba = torch.softmax(nn_your_logits, dim=1).numpy()[0]\n",
    "\n",
    "# ============================================================================\n",
    "# DISPLAY PREDICTIONS IN A COMPREHENSIVE FORMAT\n",
    "# ============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"YOUR PERSONAL HEALTH PREDICTION RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Display input data\n",
    "print(\"\\nüìã Your Input Data:\")\n",
    "print(\"-\" * 80)\n",
    "for feature, value in your_health_data.items():\n",
    "    print(f\"   {feature:<30} {value}\")\n",
    "\n",
    "# Display predictions from each model\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üîÆ PREDICTIONS FROM EACH MODEL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n1Ô∏è‚É£ Logistic Regression:\")\n",
    "print(f\"   Predicted Diagnosis: {diagnosis_labels[lr_your_pred]}\")\n",
    "print(f\"   Confidence: {lr_your_pred_proba[lr_your_pred]*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n2Ô∏è‚É£ Random Forest:\")\n",
    "print(f\"   Predicted Diagnosis: {diagnosis_labels[rf_your_pred]}\")\n",
    "print(f\"   Confidence: {rf_your_pred_proba[rf_your_pred]*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£ Neural Network:\")\n",
    "print(f\"   Predicted Diagnosis: {diagnosis_labels[nn_your_pred]}\")\n",
    "print(f\"   Confidence: {nn_your_pred_proba[nn_your_pred]*100:.2f}%\")\n",
    "\n",
    "# ============================================================================\n",
    "# BEST MODEL PREDICTION (THE ONE TO TRUST MOST)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"üèÜ BEST MODEL PREDICTION ({best_model[0].upper()})\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Use the best performing model for the final prediction\n",
    "if best_model[0] == \"Logistic Regression\":\n",
    "    best_pred = lr_your_pred\n",
    "    best_proba = lr_your_pred_proba\n",
    "elif best_model[0] == \"Random Forest\":\n",
    "    best_pred = rf_your_pred\n",
    "    best_proba = rf_your_pred_proba\n",
    "else:  # Neural Network\n",
    "    best_pred = nn_your_pred\n",
    "    best_proba = nn_your_pred_proba\n",
    "\n",
    "print(f\"\\nPredicted Diagnosis: {diagnosis_labels[best_pred]}\")\n",
    "print(f\"Confidence: {best_proba[best_pred]*100:.2f}%\") # Calculated from softmax output by taking the max value from softmax output\n",
    "\n",
    "# ============================================================================\n",
    "# SHOW TOP 3 MOST LIKELY DIAGNOSES\n",
    "# ============================================================================\n",
    "print(f\"\\nTop 3 Most Likely Diagnoses:\")\n",
    "print(f\"   {'Rank':<6} {'Diagnosis':<40} {'Probability':<12}\")\n",
    "print(f\"   {'-'*6} {'-'*40} {'-'*12}\")\n",
    "\n",
    "# Get indices of top 3 predictions sorted by probability\n",
    "top_3_indices = np.argsort(best_proba)[-3:][::-1]\n",
    "\n",
    "for rank, idx in enumerate(top_3_indices, 1):\n",
    "    diagnosis = diagnosis_labels[idx]\n",
    "    probability = best_proba[idx] * 100\n",
    "    marker = \"PREDICTION\" if idx == best_pred else \"\"\n",
    "    print(f\"   {rank:<6} {diagnosis:<40} {probability:>6.2f}%  {marker}\")\n",
    "\n",
    "\n",
    "print(\"\\n‚úÖ Personal prediction complete!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
